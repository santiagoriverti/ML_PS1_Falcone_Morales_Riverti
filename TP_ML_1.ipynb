{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "# Librerías a utilizar\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "<ipython-input-1-7a7afd9aa845>:4: DeprecationWarning: \nPyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\nbut was not found to be installed on your system.\nIf this would cause problems for you,\nplease provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n        \n  import pandas as pd\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": "# Base URL sin número de página\nbase_url = 'https://ignaciomsarmiento.github.io/GEIH2018_sample/pages/geih_page_{}.html'\n\n# Lista para almacenar los datos de todas las páginas\nall_data = []\n\n# Loop para iterar por las páginas 1 a 10\nfor page in range(1, 11):\n    # Construir la URL para cada página\n    url = base_url.format(page)\n    \n    # Enviar una solicitud HTTP\n    response = requests.get(url)\n    \n    # Verificar si la solicitud fue exitosa\n    if response.status_code == 200:\n        # Parsear el contenido con BeautifulSoup\n        soup = BeautifulSoup(response.content, 'html.parser')\n        table = soup.find('table')\n        \n        # Extraer encabezados solo en la primera iteración (ya se verifico que todas las paginas tienen el mismo encabezado)\n        if page == 1:\n            headers = [header.text.strip() for header in table.find_all('th')]\n        \n        # Extraer las filas de la tabla\n        for row in table.find_all('tr'):\n            row_data = [cell.text.strip() for cell in row.find_all('td')]\n            if row_data:\n                all_data.append(row_data)\n    else:\n        print(f\"No se pudo acceder a la página {page}\")\n\n# Crear un DataFrame con los datos recopilados\ndf = pd.DataFrame(all_data, columns=headers)\n\n# Mostrar las primeras filas del DataFrame\nprint(df.head())",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}